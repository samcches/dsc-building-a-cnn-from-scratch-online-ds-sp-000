{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CNN from Scratch - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you have background knowledge regarding how CNNs work and how to implement them via Keras, its time to practice those skills a little more independently in order to build a CNN on your own to solve a image recognition problem. In this lab, you'll practice building an image classifier from start to finish using a CNN.  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Transform images into tensors\n",
    "* Build a CNN model for image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Images\n",
    "\n",
    "The data for this lab concerns classifying lung xray images for pneumonia. The original dataset is from kaggle. We have downsampled this dataset in order to reduce training time for you when you design and fit your model to the data. ⏰ It is anticipated that this process will take approximately 1 hour to run on a standard machine, although times will vary depending on your particular computer and set up. At the end of this lab, you are welcome to try training on the complete dataset and observe the impact on the model's overall accuracy. \n",
    "\n",
    "You can find the initial downsampled dataset in a subdirectory, **chest_xray**, of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import datetime\n",
    "\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1738 images belonging to 2 classes.\n",
      "Found 4 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'chest_xray_downsampled/train'\n",
    "validation_dir = 'chest_xray_downsampled/val/'\n",
    "test_dir = 'chest_xray_downsampled/test/'\n",
    "\n",
    "#rescaling images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Model\n",
    "\n",
    "Now it's time to design your CNN! Remember a few things when doing this: \n",
    "* You should alternate convolutional and pooling layers\n",
    "* You should have later layers have a larger number of parameters in order to detect more abstract patterns\n",
    "* Add some final dense layers to add a classifier to the convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu',\n",
    "                       input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model\n",
    "\n",
    "Remember that training deep networks is resource intensive: depending on the size of the data, even a CNN with 3-4 successive convolutional and pooling layers is apt to take a hours to train on a high end laptop. Using 30 epochs and 8 layers (alternating between convolutional and pooling), our model took about 40 minutes to run on a year old macbook pro.\n",
    "\n",
    "\n",
    "If you are concerned with runtime, you may want to set your model to run the training epochs overnight.  \n",
    "\n",
    "**If you are going to run this process overnight, be sure to also script code for the following questions concerning data augmentation. Check your code twice (or more) and then set the notebook to run all, or something equivalent to have them train overnight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 222s 2s/step - loss: 0.4836 - acc: 0.7768 - val_loss: 0.7806 - val_acc: 0.7500\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 220s 2s/step - loss: 0.2758 - acc: 0.8928 - val_loss: 0.8929 - val_acc: 0.7500\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 231s 2s/step - loss: 0.1850 - acc: 0.9260 - val_loss: 1.2049 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 237s 2s/step - loss: 0.1412 - acc: 0.9469 - val_loss: 1.1821 - val_acc: 0.7500\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 225s 2s/step - loss: 0.1281 - acc: 0.9470 - val_loss: 1.6530 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 223s 2s/step - loss: 0.1024 - acc: 0.9619 - val_loss: 2.6167 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.0963 - acc: 0.9630 - val_loss: 2.3080 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 239s 2s/step - loss: 0.0881 - acc: 0.9640 - val_loss: 1.5294 - val_acc: 0.7500\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 258s 3s/step - loss: 0.0754 - acc: 0.9700 - val_loss: 1.2046 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 235s 2s/step - loss: 0.0641 - acc: 0.9775 - val_loss: 2.2546 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 221s 2s/step - loss: 0.0584 - acc: 0.9780 - val_loss: 1.6642 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 220s 2s/step - loss: 0.0584 - acc: 0.9790 - val_loss: 2.4621 - val_acc: 0.5000\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0440 - acc: 0.9820 - val_loss: 1.7983 - val_acc: 0.5000\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0401 - acc: 0.9865 - val_loss: 1.4513 - val_acc: 0.7500\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0308 - acc: 0.9895 - val_loss: 2.4813 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0362 - acc: 0.9865 - val_loss: 1.5329 - val_acc: 0.7500\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 219s 2s/step - loss: 0.0268 - acc: 0.9900 - val_loss: 0.7273 - val_acc: 0.7500\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 217s 2s/step - loss: 0.0231 - acc: 0.9915 - val_loss: 1.7845 - val_acc: 0.7500\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0197 - acc: 0.9930 - val_loss: 1.6776 - val_acc: 0.7500\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 219s 2s/step - loss: 0.0178 - acc: 0.9925 - val_loss: 2.4246 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0150 - acc: 0.9950 - val_loss: 1.7442 - val_acc: 0.7500\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 218s 2s/step - loss: 0.0128 - acc: 0.9970 - val_loss: 2.3388 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 220s 2s/step - loss: 0.0099 - acc: 0.9965 - val_loss: 5.1085 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 231s 2s/step - loss: 0.0112 - acc: 0.9965 - val_loss: 2.1088 - val_acc: 0.7500\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 241s 2s/step - loss: 0.0100 - acc: 0.9965 - val_loss: 2.3088 - val_acc: 0.7500\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 231s 2s/step - loss: 0.0054 - acc: 0.9980 - val_loss: 2.6699 - val_acc: 0.7500\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 230s 2s/step - loss: 0.0167 - acc: 0.9970 - val_loss: 2.3119 - val_acc: 0.7500\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 229s 2s/step - loss: 0.0047 - acc: 0.9985 - val_loss: 2.4186 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 219s 2s/step - loss: 0.0051 - acc: 0.9980 - val_loss: 1.0810 - val_acc: 0.7500\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 219s 2s/step - loss: 0.0050 - acc: 0.9985 - val_loss: 1.6701 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8a226f410267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chest_xray_downsampled_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Recall that data augmentation is typically always a necessary step when using a small dataset as this one which you have been provided. As such, if you haven't already, implement a data augmentation setup.\n",
    "\n",
    "**Warning: ⏰ This process took nearly 4 hours to run on a relatively new macbook pro. As such, it is recommended that you simply code the setup and compare to the solution branch, or set the process to run overnight if you do choose to actually run the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=100,\n",
    "        valdiation_data=validation_generator,\n",
    "        validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training with data augmentation took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chest_xray_downsampled_with_augmentation_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Now use the test set to perform a final evaluation on your model of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done! In this lab, you practice building your own CNN for image recognition which drastically outperformed our previous attempts using a standard deep learning model alone. In the upcoming sections, we'll continue to investigate further techniques associated with CNNs including visualizing the representations they learn and techniques to further bolster their performance when we have limited training data such as here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
